import pandas as pd
import os
import time
import json
import tempfile
import logging
import subprocess
import smtplib
from email.mime.text import MIMEText
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from typing import Optional
import librosa
import soundfile as sf
from fastapi import FastAPI
import boto3
import torch
import numpy as np
try:
    setattr(np, "NaN", np.nan)
    setattr(np, "NAN", np.nan)
except Exception:
    pass
from pyannote.audio import Pipeline
import whisper  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –ø—Ä—è–º–æ–π –∏–º–ø–æ—Ä—Ç
from gigaam_integration import GigaAMRecognizer
from dion_client import DionApiClient, DionApiError
import settings
import requests
from huggingface_hub import snapshot_download
from datetime import datetime, timedelta
import warnings
from crypto import decrypt_password
from botocore.exceptions import ClientError, EndpointConnectionError

warnings.filterwarnings("ignore")

os.makedirs(settings.LOCAL_TMP, exist_ok=True)
os.makedirs(settings.MODELS_DIR, exist_ok=True)
os.environ["HTTP_PROXY"] = ""
os.environ["HTTPS_PROXY"] = ""

# ------------------- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ -------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
# ------------------- S3 -------------------
s3 = None
if getattr(settings, "S3_ENABLED", False):
    try:
        s3 = boto3.client(
            "s3",
            endpoint_url='http://localhost:9000',
            aws_access_key_id="minioadmin",
            aws_secret_access_key="minioadmin",
            verify=False,
            region_name='us-east-1'
        )
        s3.list_buckets()
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å –∫ MinIO!")
    except (ClientError, EndpointConnectionError) as e:
        logging.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MinIO: {e}. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ S3")
        s3 = None
else:
    logging.info("S3/MinIO –æ—Ç–∫–ª—é—á–µ–Ω (S3_ENABLED=0)")

# ------------------- FastAPI -------------------
app = FastAPI()

# ------------------- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ -------------------
class PerformanceOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    @staticmethod
    def get_available_device():
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory >= 4:
                logging.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f} GB)")
                return "cuda"
        
        logging.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        return "cpu"
    
    @staticmethod
    def optimize_torch():
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º PyTorch –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        torch.set_num_threads(min(8, os.cpu_count() or 8))  # ‚Üë —É–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ç–æ–∫–∏
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            # –û—á–∏—â–∞–µ–º –∫–µ—à CUDA –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏
            torch.cuda.empty_cache()
    
    @staticmethod
    def log_processing_time(start_time, operation_name):
        """–õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        elapsed = time.time() - start_time
        logging.info(f"‚è± {operation_name}: {elapsed:.1f} —Å–µ–∫")

# ------------------- –ë—ã—Å—Ç—Ä–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è -------------------
def convert_to_wav_fast(input_path):
    """–ë—ã—Å—Ç—Ä–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ"""
    logging.info(f"‚ö° –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º {os.path.basename(input_path)}...")
    start_time = time.time()
    
    temp_path = input_path.rsplit('.', 1)[0] + "_fast.wav"
    
    try:
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã FFmpeg
        cmd = [
            "ffmpeg", "-y", 
            "-i", input_path,
            "-ar", "16000",
            "-ac", "1",
            "-acodec", "pcm_s16le",
            "-threads", "4",  # ‚Üë –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
            "-hide_banner",
            "-loglevel", "error",
            temp_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        os.replace(temp_path, input_path)
        PerformanceOptimizer.log_processing_time(start_time, "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è")
        return input_path
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise

# ------------------- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π -------------------
def load_pyannote_fast():
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ pyannote —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    logging.info("‚ö° –ó–∞–≥—Ä—É–∂–∞–µ–º pyannote...")
    start_time = time.time()
    
    try:

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
        try:
            from torch.torch_version import TorchVersion
            from pyannote.audio.core.task import Specifications
            with torch.serialization.safe_globals([TorchVersion, Specifications]):
                pipeline = Pipeline.from_pretrained(
                    settings.PYANNOTE_MODEL,
                    cache_dir=settings.MODELS_DIR,
                    local_files_only=True
                )
        except (ImportError, AttributeError, TypeError) as e:
            # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ã—á–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
            logging.debug(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É: {e}")
            pipeline = Pipeline.from_pretrained(
                settings.PYANNOTE_MODEL,
                cache_dir=settings.MODELS_DIR,
                use_auth_token="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"
            )
        
        device_type = PerformanceOptimizer.get_available_device()
        pipeline = pipeline.to(torch.device(device_type))
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        pipeline._segmentation.batch_size = 4  # ‚Üë –±–∞—Ç—á-—Å–∞–π–∑
        pipeline._segmentation.device = torch.device(device_type)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ - –∫–ª—é—á–µ–≤—ã–µ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤
        try:
            # –í PyAnnote 3.1 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç—ã
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            if hasattr(pipeline, 'clustering'):
                clustering = pipeline.clustering
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º threshold (–ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏)
                if hasattr(clustering, 'threshold'):
                    clustering.threshold = settings.DIARIZATION_CLUSTERING_THRESHOLD
                    logging.info(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {settings.DIARIZATION_CLUSTERING_THRESHOLD} (–º–µ–Ω—å—à–µ = –±–æ–ª—å—à–µ —Å–ø–∏–∫–µ—Ä–æ–≤)")
                elif hasattr(clustering, '_threshold'):
                    clustering._threshold = settings.DIARIZATION_CLUSTERING_THRESHOLD
                    logging.info(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—á–µ—Ä–µ–∑ _threshold): {settings.DIARIZATION_CLUSTERING_THRESHOLD}")
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º min_cluster_size
                if hasattr(clustering, 'min_cluster_size'):
                    clustering.min_cluster_size = settings.DIARIZATION_MIN_CLUSTER_SIZE
                    logging.info(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {settings.DIARIZATION_MIN_CLUSTER_SIZE}")
                elif hasattr(clustering, '_min_cluster_size'):
                    clustering._min_cluster_size = settings.DIARIZATION_MIN_CLUSTER_SIZE
                    logging.info(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (—á–µ—Ä–µ–∑ _min_cluster_size): {settings.DIARIZATION_MIN_CLUSTER_SIZE}")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            if hasattr(pipeline, '_segmentation'):
                seg = pipeline._segmentation
                if hasattr(seg, 'min_duration_off'):
                    seg.min_duration_off = settings.DIARIZATION_MIN_DURATION_OFF
                if hasattr(seg, 'min_duration_on'):
                    seg.min_duration_on = settings.DIARIZATION_MIN_DURATION_ON
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã pipeline –Ω–∞–ø—Ä—è–º—É—é
            if hasattr(pipeline, 'instantiate'):
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á–µ—Ä–µ–∑ instantiate
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if hasattr(pipeline, '_params') and pipeline._params:
                        if 'clustering' in pipeline._params:
                            pipeline._params['clustering']['threshold'] = settings.DIARIZATION_CLUSTERING_THRESHOLD
                            pipeline._params['clustering']['min_cluster_size'] = settings.DIARIZATION_MIN_CLUSTER_SIZE
                except Exception as e:
                    logging.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ _params: {e}")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        
        PerformanceOptimizer.log_processing_time(start_time, "–ó–∞–≥—Ä—É–∑–∫–∞ PyAnnote")
        logging.info(f"‚úÖ PyAnnote –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ (threshold={settings.DIARIZATION_CLUSTERING_THRESHOLD}, min_cluster_size={settings.DIARIZATION_MIN_CLUSTER_SIZE})")
        return pipeline
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ pyannote: {e}")
        raise

def load_whisper_fast():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Whisper"""
    logging.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Whisper...")
    start_time = time.time()
    
    try:
        device_type = PerformanceOptimizer.get_available_device()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        model = whisper.load_model(
            settings.WHISPER_MODEL, 
            device=device_type,
            download_root=settings.MODELS_DIR
        )
        
        PerformanceOptimizer.log_processing_time(start_time, "–ó–∞–≥—Ä—É–∑–∫–∞ Whisper")
        return model, device_type
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Whisper: {e}")
        raise

def load_gigaam_fast():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GigaAM"""
    logging.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º GigaAM v3...")
    start_time = time.time()
    try:
        device_type = PerformanceOptimizer.get_available_device()
        recognizer = GigaAMRecognizer(model_type=settings.GIGAAM_MODEL_TYPE, device=device_type)
        recognizer.load_model()
        PerformanceOptimizer.log_processing_time(start_time, "–ó–∞–≥—Ä—É–∑–∫–∞ GigaAM")
        return recognizer, device_type
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GigaAM: {e}")
        # raise

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
diarization_pipeline = None
gigaam_recognizer = None
whisper_model = None
device = None
# Lock –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª—è–º –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
models_lock = threading.Lock()

def initialize_models_fast():
    """–ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
    global diarization_pipeline, gigaam_recognizer, whisper_model, device
    
    if diarization_pipeline is None or (gigaam_recognizer is None and whisper_model is None):
        preload_all_models()

def preload_all_models():
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    logging.info("üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π...")
    global diarization_pipeline, gigaam_recognizer, whisper_model, device

    patch_torch_for_weights_only()

    PerformanceOptimizer.optimize_torch()
    device_type = PerformanceOptimizer.get_available_device()
    
    diarization_pipeline = load_pyannote_fast()
    # Primary: GigaAM
    gigaam_recognizer, device = load_gigaam_fast()
    # Fallback: Whisper (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è)
    # try:
    #     whisper_model, _ = load_whisper_fast()
    # except Exception as e:
    #     logging.warning(f"‚ö†Ô∏è Whisper fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    logging.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω—ã")

def _format_segments_from_gigaam(result: dict):
    result_text = " ".join(seg.get("text", "") for seg in result.get("segments", [])) or result.get("text", "")
    result_chunks = []
    for seg in result.get("segments", []):
        result_chunks.append({
            "timestamp": [seg.get("start", 0), seg.get("end", 0)],
            "text": seg.get("text", "").strip()
        })
    return result_text.strip(), result_chunks

def _transcribe_with_whisper(audio_path: str):
    if settings.TRANSCRIPTION_MODE == "quality":
        result = whisper_model.transcribe(
            audio_path,
            language="ru",
            fp16=True,
            word_timestamps=True,
            beam_size=5,
            best_of=5,
            temperature=0,
            no_speech_threshold=0.6,
            compression_ratio_threshold=2.4,
            condition_on_previous_text=False,
            logprob_threshold=-1.0,
            initial_prompt="–≠—Ç–æ –∑–∞–ø–∏—Å—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        )
    else:
        result = whisper_model.transcribe(
            audio_path,
            language="ru",
            fp16=True,
            word_timsestamps=True,
            beam_size=3,
            best_of=2,
            temperature=0.0,
            no_speech_threshold=0.6,
            compression_ratio_threshold=2.4,
            condition_on_previous_text=False,
        )
    result_text = ""
    result_chunks = []
    for segment in result["segments"]:
        result_text += segment["text"] + " "
        result_chunks.append({
            "timestamp": [segment["start"], segment["end"]],
            "text": segment["text"].strip()
        })
    return result_text.strip(), result_chunks

def _transcribe_with_gigaam(audio_path: str):
    result = gigaam_recognizer.transcribe(audio_path, language="ru")
    return _format_segments_from_gigaam(result)

# ------------------- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è -------------------
def transcribe_optimized(audio_path):
    """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º GigaAM –∏ fallback –Ω–∞ Whisper"""
    logging.info(f"üéß –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (primary: {settings.ASR_PRIMARY})...")
    start_time = time.time()

    def try_gigaam():
        if gigaam_recognizer is None:
            raise RuntimeError("GigaAM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return _transcribe_with_gigaam(audio_path)

    def try_whisper():
        if whisper_model is None:
            raise RuntimeError("Whisper –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return _transcribe_with_whisper(audio_path)

    try:
        if settings.ASR_PRIMARY == "gigaam":
            full_text, chunks = try_gigaam()
            logging.info(f"‚úÖ GigaAM —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.1f} —Å–µ–∫")
            logging.info(f"Text: {full_text} Chunk: {chunks}")
        else:
            full_text, chunks = try_whisper()
    except Exception as primary_err:
        logging.warning(f"‚ö†Ô∏è Primary ASR failed: {primary_err}. –ü—ã—Ç–∞–µ–º—Å—è fallback...")
        if settings.ASR_PRIMARY == "gigaam":
            full_text, chunks = try_whisper()
        else:
            full_text, chunks = try_gigaam()

    PerformanceOptimizer.log_processing_time(start_time, "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è")
    logging.info(f"üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–æ: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(chunks)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    return full_text, chunks

# ------------------- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ -------------------
def process_audio_optimized(audio_path, tracks=None, users_info=None):
    """–£–ù–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–ê–Ø –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    logging.info("üîπ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ...")
    start_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π (PyTorch –º–æ–¥–µ–ª–∏ thread-safe –¥–ª—è inference)
    if diarization_pipeline is None or (gigaam_recognizer is None and whisper_model is None):
        raise ValueError("–ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    # –û—á–∏—â–∞–µ–º –∫–µ—à CUDA –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    # PyTorch –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ inference –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    diarization_result = [None]
    transcription_result = [None]
    
    def run_diarization():
        # –í—ã–∑—ã–≤–∞–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å —á–µ—Ä–µ–∑ —Å–ª–æ–≤–∞—Ä—å
        diarization_params = {
            "clustering": {
                "threshold": settings.DIARIZATION_CLUSTERING_THRESHOLD,
                "min_cluster_size": settings.DIARIZATION_MIN_CLUSTER_SIZE
            },
            "segmentation": {
                "min_duration_off": settings.DIARIZATION_MIN_DURATION_OFF,
                "min_duration_on": settings.DIARIZATION_MIN_DURATION_ON
            }
        }
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–¥–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á–µ—Ä–µ–∑ kwargs
            diarization_result[0] = diarization_pipeline(audio_path, **diarization_params)
        except TypeError:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –≤—ã–∑–æ–≤
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ pipeline
            diarization_result[0] = diarization_pipeline(audio_path)
    
    def run_transcription():
        transcription_result[0] = transcribe_optimized(audio_path)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö
    t1 = threading.Thread(target=run_diarization)
    t2 = threading.Thread(target=run_transcription)
    
    t1.start()
    t2.start()
    t1.join()
    t2.join()
    
    full_text, chunks = transcription_result[0]
     # –ò—Å–ø–æ–ª—å–∑—É–µ–º –£–õ–£–ß–®–ï–ù–ù–û–ï –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    result = align_diarization_and_transcript_contextual(
        diarization_result, chunks, tracks, users_info
    )
    
    PerformanceOptimizer.log_processing_time(start_time, "–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    return result

def parse_tracks_json(json_path):
    """–ü–∞—Ä—Å–∏–Ω–≥ JSON —Ñ–∞–π–ª–∞ —Å —Ç—Ä–µ–∫–∞–º–∏ (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç)"""
    tracks = []
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    track = json.loads(line)
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã –≤ —Å–µ–∫—É–Ω–¥—ã
                    track['start_s'] = track['start_ms'] / 1000.0
                    track['end_s'] = track['end_ms'] / 1000.0
                    tracks.append(track)
                except json.JSONDecodeError as e:
                    logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ JSON: {e}")
                    continue
        logging.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(tracks)} —Ç—Ä–µ–∫–æ–≤ –∏–∑ JSON")
        return tracks
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON —Ñ–∞–π–ª–∞: {e}")
        return []

def get_user_id_for_time_advanced(tracks, start_time, end_time, previous_segments=None, speaker_history=None):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤"""
    
    start_ms = start_time * 1000
    end_ms = end_time * 1000
    segment_duration_ms = (end_time - start_time) * 1000
    
    # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –∏ –∏—Ö –æ—Ü–µ–Ω–∫–∏
    candidates = {}
    
    for track in tracks:
        track_start = track['start_ms']
        track_end = track['end_ms']
        user_id = track['user_id']
        
        # –ë–∞–∑–æ–≤–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
        overlap_start = max(start_ms, track_start)
        overlap_end = min(end_ms, track_end)
        overlap_duration = max(0, overlap_end - overlap_start)
        
        if overlap_duration == 0:
            continue
            
        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –∏ —Ç—Ä–µ–∫–∞
        overlap_pct_segment = overlap_duration / segment_duration_ms if segment_duration_ms > 0 else 0
        overlap_pct_track = overlap_duration / (track_end - track_start) if (track_end - track_start) > 0 else 0
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        overlap_score = (overlap_pct_segment * 0.6 + overlap_pct_track * 0.4)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        time_penalty = 0
        if overlap_pct_segment < 0.3:  # –ú–∞–ª–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
            time_penalty = 0.3
        elif abs(track_start - start_ms) > 2000:  # –ë–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ –ø–æ –Ω–∞—á–∞–ª—É
            time_penalty = 0.2
            
        final_score = max(0, overlap_score - time_penalty)
        
        if user_id not in candidates or final_score > candidates[user_id]['score']:
            candidates[user_id] = {
                'score': final_score,
                'overlap_pct': overlap_pct_segment,
                'track': track
            }
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    best_candidate = None
    best_score = 0
    
    for user_id, data in candidates.items():
        score = data['score']
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É –µ—Å–ª–∏ —ç—Ç–æ—Ç user_id —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ
        if speaker_history and user_id in speaker_history:
            recent_frequency = speaker_history.get(user_id, 0)
            score += min(0.2, recent_frequency * 0.1)  # –ë–æ–Ω—É—Å –¥–æ 20%
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –±–ª–∏–∑–∫–∏ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É
        if (segment_duration_ms < 3000 and previous_segments and 
            len(previous_segments) > 0):
            last_segment = previous_segments[-1]
            if (last_segment.get('user_id') == user_id and 
                start_time - last_segment['end'] < 3.0):
                score += 0.15  # –ë–æ–Ω—É—Å –∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏
        
        if score > best_score:
            best_score = score
            best_candidate = user_id
    
    # –ü–æ—Ä–æ–≥ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
    if best_candidate and best_score > 0.2:
        logging.debug(f"‚úÖ user_id {best_candidate} –¥–ª—è {start_time:.2f}-{end_time:.2f} (score: {best_score:.2f})")
        return best_candidate
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —Ö–æ—Ä–æ—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
    return get_user_id_contextual(tracks, start_time, end_time, previous_segments)

def parse_event_path_and_get_range(path: str) -> tuple[str, str, str]:
    """
    –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞:
        '/f6c28cf1-4c6c-44b4-a670-35158d9798a0/2025-11-10T11-21-00'

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        event_id, time_start, time_end
        (–≤ ISO8601 —Ñ–æ—Ä–º–∞—Ç–µ: yyyy-mm-ddTHH:MM:SSZ)
    """
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ª–µ—à–∏ –ø–æ –∫—Ä–∞—è–º
    clean = path.strip().strip("/")

    parts = clean.split("/")
    if len(parts) != 2:
        raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å: {path}")

    event_id = parts[0]
    raw_dt = parts[1]

    # –ü—Ä–∏–º–µ—Ä: 2025-11-10T11-21-00 ‚Üí –ø—Ä–∏–≤–æ–¥–∏–º –∫ datetime
    dt = datetime.strptime(raw_dt, "%Y-%m-%dT%H-%M-%S")

    # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω ¬±5 —á–∞—Å–æ–≤
    time_start = dt - timedelta(hours=2)
    time_end = dt + timedelta(hours=2)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ISO8601 –≤ UTC
    return (
        event_id,
        time_start.strftime("%Y-%m-%dT%H:%M:%SZ"),
        time_end.strftime("%Y-%m-%dT%H:%M:%SZ"),
    )

def get_user_id_contextual(tracks, start_time, end_time, previous_segments=None):
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
    
    start_ms = start_time * 1000
    end_ms = end_time * 1000
    
    # 1. –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ø–∏–∫–µ—Ä (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫)
    if previous_segments and len(previous_segments) > 0:
        last_segment = previous_segments[-1]
        last_user_id = last_segment.get('user_id')
        last_end = last_segment['end']
        
        # –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∞—è —Ä–µ–ø–ª–∏–∫–∞ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
        if (end_time - start_time < 2.0 and  # –ö–æ—Ä–æ—Ç–∫–∞—è —Ä–µ–ø–ª–∏–∫–∞
            start_time - last_end < 2.0 and   # –°—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
            last_user_id):
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç—Ä–µ–∫–∏ —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–±–ª–∏–∑–∏
            for track in tracks:
                if (track['user_id'] == last_user_id and
                    abs(track['start_ms'] - start_ms) < 3000):
                    logging.debug(f"üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –∫–æ—Ä–æ—Ç–∫–∞—è —Ä–µ–ø–ª–∏–∫–∞ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ {last_user_id}")
                    return last_user_id
    
    # 2. –ë–ª–∏–∂–∞–π—à–∏–π —Ç—Ä–µ–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ —Å–µ–≥–º–µ–Ω—Ç–∞
    closest_track = None
    min_diff = float('inf')
    
    for track in tracks:
        # –†–∞–∑–Ω–∏—Ü–∞ –ø–æ –Ω–∞—á–∞–ª—É
        start_diff = abs(track['start_ms'] - start_ms)
        # –†–∞–∑–Ω–∏—Ü–∞ –ø–æ —Å–µ—Ä–µ–¥–∏–Ω–µ
        mid_track = (track['start_ms'] + track['end_ms']) / 2
        mid_segment = (start_ms + end_ms) / 2
        mid_diff = abs(mid_track - mid_segment)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É
        diff = min(start_diff, mid_diff)
        
        if diff < min_diff:
            min_diff = diff
            closest_track = track
    
    # 3. –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–ª–∏–∑–æ—Å—Ç–∏
    if closest_track and min_diff < 3000:  # 3 —Å–µ–∫—É–Ω–¥—ã
        user_id = closest_track['user_id']
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–∫–∞
        track_duration = closest_track['end_ms'] - closest_track['start_ms']
        segment_duration = end_ms - start_ms
        
        # –ï—Å–ª–∏ —Ç—Ä–µ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        if track_duration >= segment_duration * 0.5:
            logging.debug(f"üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –±–ª–∏–∂–∞–π—à–∏–π —Ç—Ä–µ–∫ {user_id} (diff: {min_diff/1000:.2f}s)")
            return user_id
    
    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ–∫–Ω–µ
    user_durations = {}
    time_window_start = start_ms - 5000  # 5 —Å–µ–∫—É–Ω–¥ –¥–æ
    time_window_end = end_ms + 5000      # 5 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ
    
    for track in tracks:
        if (track['end_ms'] > time_window_start and 
            track['start_ms'] < time_window_end):
            
            user_id = track['user_id']
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–æ–º
            overlap_start = max(track['start_ms'], time_window_start)
            overlap_end = min(track['end_ms'], time_window_end)
            overlap_duration = max(0, overlap_end - overlap_start)
            
            user_durations[user_id] = user_durations.get(user_id, 0) + overlap_duration
    
    if user_durations:
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—É–º–º–∞—Ä–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –≤ –æ–∫–Ω–µ
        best_user = max(user_durations.items(), key=lambda x: x[1])[0]
        total_duration = sum(user_durations.values())
        confidence = user_durations[best_user] / total_duration if total_duration > 0 else 0
        
        if confidence > 0.4:  # –•–æ—Ç—è –±—ã 40% –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–∫–Ω–µ
            logging.debug(f"üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {best_user} (confidence: {confidence:.2f})")
            return best_user
    
    logging.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å user_id –¥–ª—è {start_time:.2f}-{end_time:.2f}")
    return None

def create_speaker_to_user_mapping(diarization_annotation, tracks, transcript_chunks):
    """–°–æ–∑–¥–∞–µ—Ç mapping –º–µ–∂–¥—É —Å–ø–∏–∫–µ—Ä–∞–º–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ user_id –∏–∑ tracks"""
    
    speaker_user_mapping = {}
    speaker_scores = {}
    
    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
    for turn, _, speaker in diarization_annotation.itertracks(yield_label=True):
        speaker_start = turn.start
        speaker_end = turn.end
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π user_id –∏–∑ tracks –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        best_user_id = get_user_id_for_time_advanced(tracks, speaker_start, speaker_end)
        
        if best_user_id:
            if speaker not in speaker_scores:
                speaker_scores[speaker] = {}
            
            if best_user_id not in speaker_scores[speaker]:
                speaker_scores[speaker][best_user_id] = 0
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —ç—Ç–æ–π –ø–∞—Ä—ã speaker-user_id
            duration = speaker_end - speaker_start
            speaker_scores[speaker][best_user_id] += duration
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –≤—ã–±–∏—Ä–∞–µ–º user_id —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º
    for speaker, user_scores in speaker_scores.items():
        if user_scores:
            best_user_id = max(user_scores.items(), key=lambda x: x[1])[0]
            total_time = sum(user_scores.values())
            confidence = user_scores[best_user_id] / total_time if total_time > 0 else 0
            
            if confidence >= 0.3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å 30%
                speaker_user_mapping[speaker] = best_user_id
                logging.info(f"üîó –°–ø–∏–∫–µ—Ä {speaker} ‚Üí user_id {best_user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            else:
                logging.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–ø–∏–∫–µ—Ä–∞ {speaker}: {confidence:.2f}")
    
    return speaker_user_mapping

def align_diarization_and_transcript_contextual(diarization, transcript_chunks, tracks=None, users_info=None):
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    segments = []
    speaker_history = {}
    previous_segments = []
    
    logging.info(f"üîπ Diarization type: {type(diarization)}")
    
    # –ò–ó–í–õ–ï–ö–ê–ï–ú –æ–±—ä–µ–∫—Ç Annotation –∏–∑ —Å–ø–∏—Å–∫–∞
    if isinstance(diarization, list) and len(diarization) > 0:
        logging.info("üéØ –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä–µ–∫—Ç Annotation –∏–∑ —Å–ø–∏—Å–∫–∞")
        diarization_annotation = diarization[0]
    else:
        diarization_annotation = diarization
    
    logging.info(f"üîπ Diarization annotation type: {type(diarization_annotation)}")
    
    # –°–û–ó–î–ê–ï–ú MAPPING –º–µ–∂–¥—É —Å–ø–∏–∫–µ—Ä–∞–º–∏ –∏ user_id —Å –ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–ú –ø–æ–¥—Ö–æ–¥–æ–º
    speaker_user_mapping = {}
    if tracks:
        speaker_user_mapping = create_speaker_to_user_mapping_balanced(
            diarization_annotation, tracks, transcript_chunks
        )
        logging.info(f"üîó –°–æ–∑–¥–∞–Ω–æ {len(speaker_user_mapping)} mappings —Å–ø–∏–∫–µ—Ä‚Üíuser_id")
    
    for chunk in transcript_chunks:
        start = chunk["timestamp"][0] or 0
        end = chunk["timestamp"][1] or 0
        
        if start == 0 and end == 0:
            continue
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏–∫–µ—Ä–∞ –∏–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        best_speaker = "unknown"
        max_overlap = 0
        
        try:
            for turn, _, speaker in diarization_annotation.itertracks(yield_label=True):
                overlap_start = max(start, turn.start)
                overlap_end = min(end, turn.end)
                overlap_duration = max(0, overlap_end - overlap_start)

                if overlap_duration > max_overlap:
                    max_overlap = overlap_duration
                    best_speaker = speaker
                    
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–ø–∏–∫–µ—Ä–∞: {e}")
            best_speaker = "SPEAKER_00"
        
        # –û–ü–†–ï–î–ï–õ–Ø–ï–ú user_id —á–µ—Ä–µ–∑ mapping
        user_id = None
        if tracks:
            if best_speaker in speaker_user_mapping:
                user_id = speaker_user_mapping[best_speaker]
                method = "speaker_mapping"
            else:
                # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
                user_id = get_user_id_for_time_advanced(
                    tracks, start, end, previous_segments, speaker_history
                )
                method = "time_overlap"
            
            if user_id:
                logging.debug(f"üéØ user_id {user_id} –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {start:.2f}-{end:.2f} (–º–µ—Ç–æ–¥: {method})")
                speaker_history[user_id] = speaker_history.get(user_id, 0) + 1

        segment = {
            "start": float(start),
            "end": float(end),
            "speaker": best_speaker,
            "text": chunk["text"].strip()
        }
        
        if user_id:
            #segment["user_id"] = user_id
            if users_info and user_id in users_info and users_info[user_id]:
                #segment["user_info"] = users_info[user_id]
                segment["speaker_name"] = users_info[user_id]["name"]
        
        segments.append(segment)
        previous_segments.append(segment)
        
        if len(previous_segments) > 10:
            previous_segments.pop(0)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    merged = []
    for seg in segments:
        if (merged and 
            seg["speaker"] == merged[-1]["speaker"] and
            seg.get("user_id") == merged[-1].get("user_id") and
            seg["start"] <= merged[-1]["end"] + 2.0):
            
            merged[-1]["end"] = seg["end"]
            merged[-1]["text"] += " " + seg["text"]
        else:
            merged.append(seg)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if tracks:
        user_stats = {}
        speaker_stats = {}
        
        for seg in merged:
            if 'user_id' in seg:
                user_stats[seg['user_id']] = user_stats.get(seg['user_id'], 0) + 1
            speaker_stats[seg['speaker']] = speaker_stats.get(seg['speaker'], 0) + 1
        
        logging.info(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logging.info(f"   User_id —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {user_stats}")
        logging.info(f"   –°–ø–∏–∫–µ—Ä—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {speaker_stats}")
        logging.info(f"   –°–µ–≥–º–µ–Ω—Ç–æ–≤ —Å user_id: {len([s for s in merged if 'user_id' in s])}/{len(merged)}")

    return merged

def get_users_info(user_ids, dion_client=None, event_id=None, time_start=None, time_end=None):
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö –∏–∑ DION API —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è get_event_users.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        user_ids: —Å–ø–∏—Å–æ–∫ user_id, –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∏–∑ –æ—Ç–≤–µ—Ç–∞
        dion_client: —ç–∫–∑–µ–º–ø–ª—è—Ä DionApiClient
        event_id: UUID —Å–æ–±—ã—Ç–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        time_start: –Ω–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∞ ISO8601
        time_end: –∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞ ISO8601
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        dict: {user_id: user_info –∏–ª–∏ None}
    """

    if not dion_client or not user_ids or not event_id:
        return {}

    unique_user_ids = set(user_ids)
    users_info = {}

    logging.info(f"üë§ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–æ–±—ã—Ç–∏—è {event_id} –∏–∑ DION API")

    try:
        # 1. –ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–æ–±—ã—Ç–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
        response = dion_client.get_event_users(
            event_id=event_id,
            time_start=time_start,
            time_end=time_end
        )

        event_users = response.get("users", [])
        logging.info(f"üìÅ –ü–æ–ª—É—á–µ–Ω–æ {len(event_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ DION API")

        # 2. –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ user_id –∏–∑ tracks
        for u in event_users:
            uid = u.get("user_id")
            if uid in unique_user_ids:
                users_info[uid] = {
                    "name": u.get("name"),
                    "email": u.get("email"),
                    "position": u.get("position"),
                    "sessions": u.get("sessions", [])
                }

        # 3. –î–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å—Ç–∞–≤–∏–º None
        for uid in unique_user_ids:
            if uid not in users_info:
                logging.warning(f"‚ö†Ô∏è –í DION –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ user_id={uid}")
                users_info[uid] = None

    except DionApiError as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ DION API: {e}")
        for uid in unique_user_ids:
            users_info[uid] = None
    except Exception as e:
        logging.error(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
        for uid in unique_user_ids:
            users_info[uid] = None

    return users_info



def align_diarization_and_transcript_fast(diarization, transcript_chunks, tracks=None, users_info=None):
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º user_id –∏–∑ JSON –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö"""
    segments = []

    for chunk in transcript_chunks:
        start = chunk["timestamp"][0] or 0
        end = chunk["timestamp"][1] or 0
        if start == 0 and end == 0:
            continue

        best_speaker = "unknown"
        max_overlap = 0

        for turn, _, speaker in diarization.itertracks(yield_label=True):
            overlap_start = max(start, turn.start)
            overlap_end = min(end, turn.end)
            overlap_duration = max(0, overlap_end - overlap_start)

            if overlap_duration > max_overlap:
                max_overlap = overlap_duration
                best_speaker = speaker

        # –ü–æ–ª—É—á–∞–µ–º user_id –∏–∑ JSON —Ç—Ä–µ–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        user_id = None
        if tracks:
            user_id = settings.get_user_id_for_time(tracks, start, end)

        segment = {
            "start": float(start),
            "end": float(end),
            "speaker": best_speaker,
            "text": chunk["text"].strip()
        }
        
        if user_id:
            segment["user_id"] = user_id
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            if users_info and user_id in users_info and users_info[user_id]:
                segment["user_info"] = users_info[user_id]
        
        segments.append(segment)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    merged = []
    for seg in segments:
        if (merged and 
            seg["speaker"] == merged[-1]["speaker"] and
            seg.get("user_id") == merged[-1].get("user_id") and
            seg["start"] <= merged[-1]["end"] + 1.5):  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            merged[-1]["end"] = seg["end"]
            merged[-1]["text"] += " " + seg["text"]
        else:
            merged.append(seg)

    logging.info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –≤ {len(merged)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    return merged

def format_segments_to_lines(segments):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å—Å–∏–≤ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π) –≤ –ø–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç.
    
    :param segments: list[dict] ‚Äî —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏:
                     - start (float)
                     - end (float)
                     - speaker (str, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                     - text (str)
    :return: str ‚Äî –≥–æ—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
               [00:00,72 ‚Äî 00:10,98] –ò–º—è: –¢–µ–∫—Å—Ç
    """
    def format_time(seconds):
        mins = int(seconds // 60)
        secs = seconds % 60
        return f"{mins:02}:{secs:05.2f}".replace('.', ',')

    lines = []
    for seg in segments:
        speaker = seg.get('speaker_name', '').strip() or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π"
        text = seg.get('text', '').strip()

        line = f"{speaker}: {text}"
        lines.append(line)

    return '\n'.join(lines)

def send_email(subject: str, body: str, to_email: str = None):
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ email
    
    Args:
        subject: –¢–µ–º–∞ –ø–∏—Å—å–º–∞
        body: –¢–µ–ª–æ –ø–∏—Å—å–º–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å HTML)
        to_email: Email –ø–æ–ª—É—á–∞—Ç–µ–ª—è (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
    """
    recipient_email = to_email or settings.EMAIL_TO
    
    msg = MIMEText(body, 'html' if '<' in body else 'plain', 'utf-8')
    msg['Subject'] = subject
    msg['From'] = settings.EMAIL_FROM
    msg['To'] = recipient_email
    
    with smtplib.SMTP(settings.EMAIL_HOST, settings.EMAIL_PORT) as server:
        if settings.EMAIL_USE_TLS:
            server.starttls()
        server.login(settings.EMAIL_USER, decrypt_password(settings.EMAIL_PASS))
        server.send_message(msg)
    
    logging.info(f"üìß Email –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω: {subject} -> {recipient_email}")
        
    

def create_speaker_to_user_mapping_balanced(diarization_annotation, tracks, transcript_chunks):
    """–ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å user_id –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è"""
    
    speaker_user_mapping = {}
    speaker_scores = {}
    
    logging.info("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ mapping –º–µ–∂–¥—É —Å–ø–∏–∫–µ—Ä–∞–º–∏ –∏ user_id...")
    
    # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
    for turn, _, speaker in diarization_annotation.itertracks(yield_label=True):
        speaker_start = turn.start
        speaker_end = turn.end
        
        # –ò—â–µ–º –ª—É—á—à–∏–π user_id –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        best_user_id = get_user_id_for_time_advanced(tracks, speaker_start, speaker_end)
        
        if best_user_id:
            if speaker not in speaker_scores:
                speaker_scores[speaker] = {}
            
            # –°—É–º–º–∏—Ä—É–µ–º –≤—Ä–µ–º—è –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Å–ø–∏–∫–µ—Ä-user_id
            duration = speaker_end - speaker_start
            speaker_scores[speaker][best_user_id] = speaker_scores[speaker].get(best_user_id, 0) + duration
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    for speaker, user_scores in speaker_scores.items():
        total_time = sum(user_scores.values())
        logging.info(f"üìä –°–ø–∏–∫–µ—Ä {speaker}: {len(user_scores)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –æ–±—â–µ–µ –≤—Ä–µ–º—è {total_time:.1f}—Å")
        for user_id, time in user_scores.items():
            confidence = time / total_time
            logging.info(f"   üë§ {user_id}: {time:.1f}—Å ({confidence:.1%})")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    speaker_candidates = {}
    for speaker, user_scores in speaker_scores.items():
        total_time = sum(user_scores.values())
        candidates = []
        for user_id, time in user_scores.items():
            confidence = time / total_time if total_time > 0 else 0
            candidates.append((user_id, confidence, time))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π)
        candidates.sort(key=lambda x: x[1], reverse=True)
        speaker_candidates[speaker] = candidates
        
        if candidates:
            best_user, best_conf, best_time = candidates[0]
            logging.info(f"üéØ –°–ø–∏–∫–µ—Ä {speaker}: –ª—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç {best_user} ({best_conf:.1%})")
    
    # –ú–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    
    assigned_users = set()  # –£–∂–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ user_id
    
    # –†–∞—É–Ω–¥ 1: –Ω–∞–∑–Ω–∞—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã —Å –í–´–°–û–ö–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 0.7)
    logging.info("üîπ –†–∞—É–Ω–¥ 1: –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 70%)")
    for speaker, candidates in speaker_candidates.items():
        if speaker in speaker_user_mapping:
            continue
            
        for user_id, confidence, time in candidates:
            if user_id not in assigned_users and confidence > 0.7:
                speaker_user_mapping[speaker] = user_id
                assigned_users.add(user_id)
                logging.info(f"‚úÖ [–†–∞—É–Ω–¥ 1] {speaker} ‚Üí {user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
                break
    
    # –†–∞—É–Ω–¥ 2: –Ω–∞–∑–Ω–∞—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã —Å –°–†–ï–î–ù–ï–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 0.5)
    logging.info("üîπ –†–∞—É–Ω–¥ 2: –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 50%)")
    for speaker, candidates in speaker_candidates.items():
        if speaker in speaker_user_mapping:
            continue
            
        for user_id, confidence, time in candidates:
            if user_id not in assigned_users and confidence > 0.5:
                speaker_user_mapping[speaker] = user_id
                assigned_users.add(user_id)
                logging.info(f"‚úÖ [–†–∞—É–Ω–¥ 2] {speaker} ‚Üí {user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
                break
    
    # –†–∞—É–Ω–¥ 3: –Ω–∞–∑–Ω–∞—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã —Å –ú–ò–ù–ò–ú–ê–õ–¨–ù–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 0.3)
    logging.info("üîπ –†–∞—É–Ω–¥ 3: –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (> 30%)")
    for speaker, candidates in speaker_candidates.items():
        if speaker in speaker_user_mapping:
            continue
            
        for user_id, confidence, time in candidates:
            if user_id not in assigned_users and confidence > 0.3:
                speaker_user_mapping[speaker] = user_id
                assigned_users.add(user_id)
                logging.info(f"‚úÖ [–†–∞—É–Ω–¥ 3] {speaker} ‚Üí {user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
                break
    
    # –†–∞—É–Ω–¥ 4: –£–õ–£–ß–®–ï–ù–ù–´–ô - –∏—â–µ–º –ª—é–±–æ–≥–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞, –¥–∞–∂–µ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
    logging.info("üîπ –†–∞—É–Ω–¥ 4: –ü–æ–∏—Å–∫ –ª—é–±–æ–≥–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
    for speaker, candidates in speaker_candidates.items():
        if speaker in speaker_user_mapping:
            continue
            
        # –ò—â–µ–º –ø–µ—Ä–≤–æ–≥–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
        assigned = False
        for user_id, confidence, time in candidates:
            if user_id not in assigned_users:
                speaker_user_mapping[speaker] = user_id
                assigned_users.add(user_id)
                logging.info(f"‚úÖ [–†–∞—É–Ω–¥ 4] {speaker} ‚Üí {user_id} (—Å–≤–æ–±–æ–¥–Ω—ã–π, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
                assigned = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ, –±–µ—Ä–µ–º –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        if not assigned and candidates:
            # –ò—â–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—É—â–∏—Ö –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π
            user_assignment_count = {}
            for user_id, confidence, time in candidates:
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —ç—Ç–æ—Ç user_id —É–∂–µ –Ω–∞–∑–Ω–∞—á–µ–Ω
                count = sum(1 for s, uid in speaker_user_mapping.items() if uid == user_id)
                user_assignment_count[user_id] = count
            
            # –ë–µ—Ä–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π
            best_user_id = min(user_assignment_count.items(), key=lambda x: x[1])[0]
            confidence = next((conf for uid, conf, t in candidates if uid == best_user_id), 0)
            current_count = user_assignment_count[best_user_id]
            
            speaker_user_mapping[speaker] = best_user_id
            
            if current_count > 0:
                logging.warning(f"‚ö†Ô∏è [–†–∞—É–Ω–¥ 4] {speaker} ‚Üí {best_user_id} (–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ #{current_count + 1}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
            else:
                assigned_users.add(best_user_id)
                logging.info(f"‚úÖ [–†–∞—É–Ω–¥ 4] {speaker} ‚Üí {best_user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logging.info("üìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê MAPPING:")
    user_speaker_count = {}
    for speaker, user_id in speaker_user_mapping.items():
        user_speaker_count[user_id] = user_speaker_count.get(user_id, 0) + 1
        confidence = next((conf for uid, conf, t in speaker_candidates[speaker] if uid == user_id), 0)
        logging.info(f"   {speaker} ‚Üí {user_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    duplicates = {user: count for user, count in user_speaker_count.items() if count > 1}
    if duplicates:
        logging.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è user_id: {duplicates}")
    else:
        logging.info("‚úÖ –í—Å–µ user_id –Ω–∞–∑–Ω–∞—á–µ–Ω—ã —É–Ω–∏–∫–∞–ª—å–Ω–æ!")
    
    logging.info(f"üéØ –ò—Ç–æ–≥: {len(speaker_user_mapping)} —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ —Å {len(assigned_users)} user_id")
    
    return speaker_user_mapping

def process_directory(s3_prefix):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å UUID/timestamp —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    local_audio_path = None
    local_json_path = None
    total_start_time = time.time()
    
    try:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–µ–π)
        with models_lock:
            initialize_models_fast()
        
        logging.info(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {s3_prefix}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        response = s3.list_objects_v2(Bucket=settings.S3_BUCKET, Prefix=s3_prefix)
        files = response.get("Contents", [])
        
        # –ò—â–µ–º mp4 –∏ json —Ñ–∞–π–ª—ã
        mp4_file = None
        json_file = None
        
        for file_obj in files:
            key = file_obj["Key"]
            filename = os.path.basename(key)
            ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''
            
            if ext == 'mp4':
                mp4_file = key
            elif ext == 'json':
                json_file = key
        
        if not mp4_file:
            logging.warning(f"‚ö†Ô∏è MP4 —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {s3_prefix}")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º MP4 —Ñ–∞–π–ª
        local_audio_path = os.path.join(settings.LOCAL_TMP, str(hash(mp4_file)) + ".mp4")
        logging.info(f"‚¨áÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º MP4 —Ñ–∞–π–ª: {mp4_file}")
        s3.download_file(settings.S3_BUCKET, mp4_file, local_audio_path)
        
        file_size_mb = os.path.getsize(local_audio_path) / (1024 * 1024)
        logging.info(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.1f} MB")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª, –µ—Å–ª–∏ –µ—Å—Ç—å
        tracks = None
        if json_file:
            local_json_path = os.path.join(settings.LOCAL_TMP, str(hash(json_file)) + ".json")
            logging.info(f"‚¨áÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª: {json_file}")
            s3.download_file(settings.S3_BUCKET, json_file, local_json_path)
            tracks = parse_tracks_json(local_json_path)
        else:
            logging.info("‚ÑπÔ∏è JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ user_id")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
        ext = local_audio_path.rsplit('.', 1)[1].lower()
        if ext != "wav":
            local_audio_path = convert_to_wav_fast(local_audio_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        duration = librosa.get_duration(filename=local_audio_path)
        logging.info(f"‚è± –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {duration:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö –∏–∑ DION API, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        users_info = {}
        dion_client = None
        owner_email = None
        slug = None

        # –ë–µ—Ä—ë–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ –¥–∞—Ç–µ —Å–æ–±—ã—Ç–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä ¬±5 —á–∞—Å–æ–≤)
        event_id, time_start, time_end = parse_event_path_and_get_range(s3_prefix)
        if settings.DION_API_ENABLED and tracks:
            try:
                dion_client = DionApiClient(access_token=decrypt_password(settings.DION_ACCESS_TOKEN))
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ user_id –∏–∑ —Ç—Ä–µ–∫–æ–≤
                user_ids = [track['user_id'] for track in tracks if 'user_id' in track]


                if user_ids:
                    users_info = get_users_info(
                        user_ids=user_ids,
                        dion_client=dion_client,
                        event_id=event_id,
                        time_start=time_start,
                        time_end=time_end
                    )

                 # –ò–ó–í–õ–ï–ö–ê–ï–ú UUID —Å–æ–±—ã—Ç–∏—è –∏–∑ s3_prefix
                # s3_prefix –∏–º–µ–µ—Ç —Ñ–æ—Ä–º–∞—Ç: uuid/timestamp/
                event_uuid = extract_event_uuid_from_s3_prefix(s3_prefix)
                
                if event_uuid:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–±—ã—Ç–∏–∏
                    event_data = dion_client.get_event_data_by_id(event_uuid)
                    slug = event_data.get("link_settings", {}).get("slug","")
                    logging.info(f"üìã –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è: {event_uuid}")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º owner_email –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    if event_data:
                       owner_email = event_data.get('owner_email')
                       logging.info(f"üë§ Owner email: {owner_email}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DION API –∫–ª–∏–µ–Ω—Ç–∞: {e}")
                users_info = {}
        
        # –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —Ç—Ä–µ–∫–∞–º–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
        result_segments = process_audio_optimized(local_audio_path, tracks, users_info)
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º DION API –∫–ª–∏–µ–Ω—Ç
        if dion_client:
            try:
                dion_client.close()
            except Exception:
                pass
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        result_json = json.dumps({
            "status": "success",
            "segments": result_segments,
            "total_duration": duration,
            "directory": s3_prefix,
            "processing_time": round(time.time() - total_start_time, 1)
        }, ensure_ascii=False, indent=2)
        #logging.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result_json}")
        
          # –û–¢–ü–†–ê–í–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê –ù–ê –ü–û–ß–¢–£
        if owner_email:
            logging.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ –ø–æ—á—Ç—É {format_segments_to_lines(result_segments)}")
            send_email(f"—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ dion-–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∑–∞ {iso8601_to_dd_mm_yyyy(time_start)} –∫–æ–º–Ω–∞—Ç–∞ {slug!r}", format_segments_to_lines(result_segments), to_email=owner_email)
        else:
            raise Exception(f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ—á—Ç–∞ –≤–ª–∞–¥–µ–ª—å–Ω–∞—è –ø–æ {s3_prefix}")
           
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ timestamp –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏–∑ S3 (–Ω–µ –≤—Å—é UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é)
        # s3_prefix –∏–º–µ–µ—Ç —Ñ–æ—Ä–º–∞—Ç: uuid/timestamp/
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø—Ä–µ—Ñ–∏–∫—Å –∑–∞–∫–∞–Ω–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ '/'
            delete_prefix = s3_prefix if s3_prefix.endswith('/') else s3_prefix + '/'
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –¥–∞–Ω–Ω—ã–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            objects_to_delete = []
            paginator = s3.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=settings.S3_BUCKET, Prefix=delete_prefix):
                if 'Contents' in page:
                    objects_to_delete.extend([{'Key': obj['Key']} for obj in page['Contents']])
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã –ø–∞—á–∫–æ–π
            if objects_to_delete:
                response = s3.delete_objects(
                    Bucket=settings.S3_BUCKET,
                    Delete={'Objects': objects_to_delete}
                )
                deleted_count = len(response.get('Deleted', []))
            else:
                deleted_count = 0
            
            logging.info(f"üóë –£–¥–∞–ª–µ–Ω–∞ timestamp –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {s3_prefix} –∏–∑ S3 (—É–¥–∞–ª–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {deleted_count})")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ S3: {e}")
        
        total_time = time.time() - total_start_time
        speed_ratio = duration / total_time if total_time > 0 else 0
        logging.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f} —Å–µ–∫ ({speed_ratio:.2f}x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)")
    
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {s3_prefix}: {e}")
        send_email(f"–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ {s3_prefix}", str(e))
    finally:
        if local_audio_path and os.path.exists(local_audio_path):
            os.remove(local_audio_path)
        if local_json_path and os.path.exists(local_json_path):
            os.remove(local_json_path)
        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


def iso8601_to_dd_mm_yyyy(iso_date_str: str) -> str:
    dt = datetime.strptime(iso_date_str, "%Y-%m-%dT%H:%M:%SZ")
    return dt.strftime("%d.%m.%Y")

def extract_event_uuid_from_s3_prefix(s3_prefix: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç UUID —Å–æ–±—ã—Ç–∏—è –∏–∑ S3 –ø—É—Ç–∏.
    –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: uuid/timestamp/ –∏–ª–∏ uuid/timestamp
    
    Args:
        s3_prefix: –ü—É—Ç—å –≤ S3, –Ω–∞–ø—Ä–∏–º–µ—Ä "f6c28cf1-4c6c-44b4-a670-35158d9798a1/2025-11-10T11-21-00/"
    
    Returns:
        UUID —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å
    """
    try:
        # –£–±–∏—Ä–∞–µ–º trailing slash –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ /
        clean_prefix = s3_prefix.rstrip('/')
        parts = clean_prefix.split('/')
        
        # UUID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç—å—é –∏ –∏–º–µ—Ç—å –¥–ª–∏–Ω—É 36 —Å–∏–º–≤–æ–ª–æ–≤
        if len(parts) >= 1:
            potential_uuid = parts[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ UUID (36 —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–µ—Ñ–∏—Å—ã)
            if (len(potential_uuid) == 36 and 
                potential_uuid.count('-') == 4 and
                all(part.isalnum() or part == '' for part in potential_uuid.split('-'))):
                
                logging.info(f"üéØ –ò–∑–≤–ª–µ—á–µ–Ω UUID –∏–∑ S3 –ø—Ä–µ—Ñ–∏–∫—Å–∞: {potential_uuid}")
                return potential_uuid
            else:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å UUID –∏–∑ S3 –ø—Ä–µ—Ñ–∏–∫—Å–∞: {s3_prefix}")
                return None
        else:
            logging.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç S3 –ø—Ä–µ—Ñ–∏–∫—Å–∞: {s3_prefix}")
            return None
            
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ UUID –∏–∑ {s3_prefix}: {e}")
        return None


def send_email_to_owner(owner_email: str, event_uuid: str, result_data: dict, s3_prefix: str, date: str):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ email –≤–ª–∞–¥–µ–ª—å—Ü–∞ —Å–æ–±—ã—Ç–∏—è.
    
    Args:
        owner_email: Email –≤–ª–∞–¥–µ–ª—å—Ü–∞ —Å–æ–±—ã—Ç–∏—è
        event_uuid: UUID —Å–æ–±—ã—Ç–∏—è
        result_data: –î–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        s3_prefix: –ò—Å—Ö–æ–¥–Ω—ã–π S3 –ø—Ä–µ—Ñ–∏–∫—Å
    """
    try:
       

        for segment in segments:
            speaker = segment.get("speaker", "unknown")
            user_id = segment.get("user_id")

            speaker_stats[speaker] = speaker_stats.get(speaker, 0) + 1
            if user_id:
                user_stats[user_id] = user_stats.get(user_id, 0) + 1

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–º—É –ø–∏—Å—å–º–∞
        subject = f"—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ dion-–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∑–∞ {date}"

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–ª–æ –ø–∏—Å—å–º–∞ (HTML)
        body = f""""""

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        for speaker, count in sorted(speaker_stats.items(), key=lambda x: x[1], reverse=True):
            user_id = next((seg.get('user_id') for seg in segments if seg.get('speaker') == speaker), None)
            user_info = next((seg.get('user_info') for seg in segments if seg.get('speaker') == speaker), None)

            user_display = ""
            if user_info:
                user_display = f" ({user_info.get('name', user_info.get('email', user_id))})"
            elif user_id:
                user_display = f" (user_id: {user_id})"

            body += f"<li>{speaker}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤{user_display}</li>\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        body += f"""
<h3>üìù –ü—Ä–∏–º–µ—Ä—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–ø–µ—Ä–≤—ã–µ 5 —Å–µ–≥–º–µ–Ω—Ç–æ–≤):</h3>
"""

        for i, segment in enumerate(segments[:5]):
            speaker = segment.get('speaker', 'unknown')
            text = segment.get('text', '')


            body += f"""
<div style="margin-bottom: 10px; padding: 10px; background: #f5f5f5; border-radius: 5px;">
    <strong> {speaker}: {text}
</div>
"""

        

        body += f"""
<hr/>
<p><em>–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏—Å—Ç–µ–º–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ Dion</em></p>
"""

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–∏—Å—å–º–æ —á–µ—Ä–µ–∑ –æ–±—â–∏–π –º–µ—Ç–æ–¥
        send_email(subject=subject, body=body, to_email=owner_email)
        logging.info(f"üìß –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ email –≤–ª–∞–¥–µ–ª—å—Ü–∞: {owner_email}")

    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ email –≤–ª–∞–¥–µ–ª—å—Ü—É {owner_email}: {e}")
        # –†–µ–∑–µ—Ä–≤–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
        fallback_subject = f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è {s3_prefix}"
        fallback_body = json.dumps(result_data, ensure_ascii=False, indent=2)
        send_email(subject=fallback_subject, body=fallback_body)

# ------------------- –§–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª -------------------
async def background_loop():
    if s3 is None:
        logging.info("S3 –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —Ñ–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è")
        return
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.empty_cache()
    print("PyTorch version:", torch.__version__)
    print("CUDA available:", torch.cuda.is_available())
    print("CUDA version:", torch.version.cuda)
    print("GPU name:", torch.cuda.get_device_name(0))
    """–§–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π UUID/timestamp —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    processed_dirs = set()
    executor = ThreadPoolExecutor(max_workers=settings.PARALLEL_WORKERS)
    
    logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ü–∏–∫–ª–∞ —Å {settings.PARALLEL_WORKERS} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏")
    
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è (UUID)
            response = s3.list_objects_v2(Bucket=settings.S3_BUCKET, Delimiter='/')
            uuid_prefixes = response.get("CommonPrefixes", [])
            
            #if uuid_prefixes:
            #    logging.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(uuid_prefixes)}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            directories_to_process = []
            
            # –î–ª—è –∫–∞–∂–¥–æ–π UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏—â–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ timestamp –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            for uuid_prefix_obj in uuid_prefixes:
                uuid_prefix = uuid_prefix_obj["Prefix"]
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (timestamp)
                timestamp_response = s3.list_objects_v2(
                    Bucket=settings.S3_BUCKET, 
                    Prefix=uuid_prefix, 
                    Delimiter='/'
                )
                timestamp_prefixes = timestamp_response.get("CommonPrefixes", [])
                
                if not timestamp_prefixes:
                    #logging.info(f"üì≠ –í UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {uuid_prefix} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ timestamp –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
                    continue
                
                for timestamp_prefix_obj in timestamp_prefixes:
                    timestamp_prefix = timestamp_prefix_obj["Prefix"]
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    if timestamp_prefix in processed_dirs:
                        logging.info(f"‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {timestamp_prefix}")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: UUID/timestamp/
                    parts = timestamp_prefix.rstrip('/').split('/')
                    if len(parts) >= 2:
                        uuid_part = parts[-2]
                        timestamp_part = parts[-1]

                        if uuid_part not in settings.UUID_WHITELIST:
                            #logging.info(f"‚ö†Ô∏è UUID {uuid_part} –Ω–µ –≤ –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ UUID –∏ timestamp
                        if len(uuid_part) == 36 and 'T' in timestamp_part:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                            dir_response = s3.list_objects_v2(
                                Bucket=settings.S3_BUCKET, 
                                Prefix=timestamp_prefix
                            )
                            dir_files = dir_response.get("Contents", [])
                            
                            if not dir_files:
                                logging.info(f"üì≠ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {timestamp_prefix} –ø—É—Å—Ç–∞")
                                continue
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ MP4 —Ñ–∞–π–ª–∞
                            has_mp4 = any(
                                os.path.basename(f["Key"]).lower().endswith('.mp4') 
                                for f in dir_files
                            )
                            
                            if has_mp4:
                                directories_to_process.append(timestamp_prefix)
                                logging.info(f"üé¨ –ù–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {timestamp_prefix}")
                            else:
                                logging.info(f"üì≠ –í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {timestamp_prefix} –Ω–µ—Ç MP4 —Ñ–∞–π–ª–∞")
                        else:
                            logging.info(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {timestamp_prefix} –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç—É UUID/timestamp")
                    else:
                        logging.info(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {timestamp_prefix}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            if directories_to_process:
                logging.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(directories_to_process)} –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
                futures = []
                for timestamp_prefix in directories_to_process:
                    future = executor.submit(process_directory, timestamp_prefix)
                    futures.append((future, timestamp_prefix))
                
                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
                for future, timestamp_prefix in futures:
                    try:
                        future.result(timeout=3600)  # –¢–∞–π–º–∞—É—Ç 1 —á–∞—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
                        processed_dirs.add(timestamp_prefix)
                        logging.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞: {timestamp_prefix}")
                    except Exception as e:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {timestamp_prefix}: {e}")
            else:
                # –õ–æ–≥–∏—Ä—É–µ–º, –ø–æ—á–µ–º—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                if uuid_prefixes:
                    logging.debug(f"‚ÑπÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(uuid_prefixes)} UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –Ω–æ –Ω–µ—Ç –Ω–æ–≤—ã—Ö timestamp –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                else:
                    logging.debug("‚ÑπÔ∏è –ù–µ—Ç UUID –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
        
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ü–∏–∫–ª–µ: {e}")
            import traceback
            logging.error(traceback.format_exc())
            await asyncio.sleep(30)
        
        await asyncio.sleep(settings.CHECK_INTERVAL)

def patch_torch_for_weights_only():
    """–ü–∞—Ç—á –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.6+"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é PyTorch
        torch_version = torch.__version__
        logging.info(f"üîß PyTorch version: {torch_version}")
        
        # –î–ª—è –≤–µ—Ä—Å–∏–π 2.6 –∏ –≤—ã—à–µ
        if tuple(map(int, torch_version.split('.')[:2])) >= (2, 6):
            logging.info("üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è PyTorch 2.6+ (weights_only=False)")
            
            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            original_load = torch.load
            
            def patched_load(f, map_location=None, pickle_module=None, 
                           weights_only=None, **kwargs):
                # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º weights_only=False –¥–ª—è –º–æ–¥–µ–ª–µ–π
                return original_load(f, map_location=map_location, 
                                   pickle_module=pickle_module,
                                   weights_only=False, **kwargs)
            
            torch.load = patched_load
            logging.info("‚úÖ –ü–∞—Ç—á –ø—Ä–∏–º–µ–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ç—á –¥–ª—è PyTorch: {e}")

@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    logging.info("üöÄ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    preload_all_models()
    if s3 is not None:
        logging.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ü–∏–∫–ª–∞...")
        asyncio.create_task(background_loop())
    else:
        logging.info("‚è≠ –§–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª –Ω–µ –∑–∞–ø—É—â–µ–Ω: S3 –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

@app.get("/")
def read_root():
    return {"status": "ok", "optimized": True, "version": "2.0"}

@app.get("/health")
def health_check():
    device_type = PerformanceOptimizer.get_available_device()
    return {
        "status": "healthy",
        "device": device_type,
        "models_loaded": diarization_pipeline is not None and whisper_model is not None,
        "parallel_workers": settings.PARALLEL_WORKERS,
        "transcription_mode": settings.TRANSCRIPTION_MODE,
        "whisper_model": settings.WHISPER_MODEL,
        "pyannote_model": settings.PYANNOTE_MODEL,
        "diarization": {
            "clustering_threshold": settings.DIARIZATION_CLUSTERING_THRESHOLD,
            "min_cluster_size": settings.DIARIZATION_MIN_CLUSTER_SIZE,
            "min_duration_off": settings.DIARIZATION_MIN_DURATION_OFF,
            "min_duration_on": settings.DIARIZATION_MIN_DURATION_ON
        }
    }

if __name__ == "__main__":
    if s3 is not None:
        asyncio.run(background_loop())
    else:
        logging.info("S3 –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –≤—ã—Ö–æ–¥ –±–µ–∑ –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ü–∏–∫–ª–∞")
